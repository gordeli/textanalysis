{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/textanalysis/blob/master/04_Content_Analysis_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Fundamentals of Text Analysis for User Generated Content @ EDHEC, 2021\n",
    "\n",
    "# Part 4: Content Analysis\n",
    "\n",
    "[<- Previous: Data Collection](https://colab.research.google.com/github/gordeli/textanalysis/blob/master/colab/03_Data_Collection_DS3Text.ipynb)\n",
    "\n",
    "[-> Next: Text Embeddings](https://colab.research.google.com/github/gordeli/textanalysis/blob/master/colab/05_Text_Text_Embeddings_DS3Text.ipynb)\n",
    "\n",
    "Dates: February 8 - 15, 2021\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKVEnPi34qj4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Vanya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Vanya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Vanya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Vanya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/Vanya/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/cf/87b25b265d23498b2b70ce873495cf7ef91394c4baff240210e26f3bc18a/gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 1.0MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/54/01525817b6f31533d308968b814999f7e666b2234f39a55cbe5de7c1ff99/smart_open-4.1.2-py3-none-any.whl (111kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 3.2MB/s \n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.1.2\n",
      "Collecting pyLDAvis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/7f/89778b3eb9c84c64b31567d53ad9e373b253244d45980fed01f562441aed/pyLDAvis-3.2.0.tar.gz (1.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.7MB 3.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: numexpr in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (2.6.9)\n",
      "Requirement already satisfied: future in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.17.1)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=0.17.0 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Vanya/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas>=0.17.0->pyLDAvis) (1.12.0)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/Vanya/Library/Caches/pip/wheels/5e/62/9d/e2d11b7e09f85508b7be451fa35a2bce4b12c66c10b8bcb182\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: funcy, pyLDAvis\n",
      "Successfully installed funcy-1.15 pyLDAvis-3.2.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0872039eb011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# for uploading data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# downloading values lexicon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# Natural Language Toolkit (https://www.nltk.org/)\n",
    "import nltk\n",
    "\n",
    "# download punctuation related NLTK functions\n",
    "# (needed for sent_tokenize())\n",
    "nltk.download('punkt')\n",
    "# download NLKT part-of-speech tagger\n",
    "# (needed for pos_tag())\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# download wordnet\n",
    "# (needed for lemmatization)\n",
    "nltk.download('wordnet')\n",
    "# download stopword lists\n",
    "# (needed for stopword removal)\n",
    "nltk.download('stopwords')\n",
    "# dictionary of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# numpy: matrix library for Python\n",
    "import numpy as np\n",
    "\n",
    "!pip install -U gensim\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "# for loading data\n",
    "import sklearn.datasets\n",
    "# for LDA visualization\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# for uploading data files\n",
    "from google.colab import files\n",
    "\n",
    "# downloading values lexicon\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/lexicon_1_0/values_lexicon.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/christian_500.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/business_500.txt\n",
    "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/college_500.txt\n",
    "\n",
    "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # create a place to store (word, pos_tag) tuples\n",
    "    words_and_pos_tags = []\n",
    "    \n",
    "    # get all words and pos tags\n",
    "    for sentence in sentences:\n",
    "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        \n",
    "    # load the lemmatizer\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize the words\n",
    "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
    "              (word,pos) in words_and_pos_tags]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set([])\n",
    "    if remove_stop_words:\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    # only get lemmas that aren't in these lists\n",
    "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
    "                      if lemma not in all_removal_tokens and \\\n",
    "                      re.match(r\"^\\w+$\",lemma)]\n",
    "    \n",
    "    # return the frequency distribution object\n",
    "    return nltk.probability.FreqDist(content_lemmas)\n",
    "    \n",
    "def docs2matrix(document_list):\n",
    "    \n",
    "    # use the vocab2index idea from before\n",
    "    vocab2index = {}\n",
    "    \n",
    "    # load the stopword list for English\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_words |= set(['from', 'subject', 're', 'edu', 'use'])\n",
    "    \n",
    "    # add punctuation to the set of things to remove\n",
    "    all_removal_tokens = stop_words | set(string.punctuation)\n",
    "    \n",
    "    # bonus: also add some custom double-quote tokens to this set\n",
    "    all_removal_tokens |= set([\"''\",\"``\"])\n",
    "    \n",
    "    vocab2index = {}\n",
    "    latest_index = 0\n",
    "\n",
    "    lfs = []\n",
    "    # this should be a nice starting point\n",
    "    for doc in document_list:\n",
    "        lf = text_to_lemma_frequencies(doc,all_removal_tokens)\n",
    "        for token in lf.keys():\n",
    "            if token not in vocab2index:\n",
    "                vocab2index[token] = latest_index\n",
    "                latest_index += 1\n",
    "                \n",
    "        lfs.append(lf)\n",
    "    \n",
    "    # create the zeros matrix\n",
    "    corpus_matrix = np.zeros((len(lfs), len(vocab2index)))\n",
    "    \n",
    "    for row, lf in enumerate(lfs):\n",
    "        for token, frequency in lf.items():\n",
    "            column = vocab2index[token]\n",
    "            corpus_matrix[row][column] = frequency\n",
    "    \n",
    "    return corpus_matrix, vocab2index\n",
    "\n",
    "    \n",
    "# Lemmatization -- redefining this here to make\n",
    "# code block more self-contained\n",
    "def lookup_pos(pos):\n",
    "    pos_first_char = pos[0].lower()\n",
    "    if pos_first_char in 'nv':\n",
    "        return pos_first_char\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "            \n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJv8G-gS7GId"
   },
   "source": [
    "---\n",
    "\n",
    "## Content Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_95J7lYM97q9"
   },
   "source": [
    "- Now that we have some real data, what are some ways that we can explore what's in it?\n",
    "    - How can we answer the basic question: *What are people talking about in this corpus?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6xMWv0M7HyH"
   },
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKxxCNGH8o5r"
   },
   "source": [
    "- Load a corpus matrix, like the ones we created earlier, into gensim's corpus object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ERVLeUv7Ka3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'docs2matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf061d2af2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                               categories=categories).data\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# using the function we wrote before, but modified to also return the vocab2index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcorpus_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs2matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsgroups_train_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# reverse this dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs2matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# this time, let's load all documents in the 20news dataset from these categories\n",
    "categories = ['soc.religion.christian', 'rec.autos', 'talk.politics.misc', \\\n",
    "              'rec.sport.baseball', 'comp.sys.ibm.pc.hardware']\n",
    "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train', \\\n",
    "                                              categories=categories).data\n",
    "# using the function we wrote before, but modified to also return the vocab2index\n",
    "corpus_matrix, word2id = docs2matrix(newsgroups_train_all)\n",
    "# reverse this dictionary\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)\n",
    "print(\"Loaded\",len(corpus),\"documents into a Gensim corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yO59RFk_o2r"
   },
   "source": [
    "- Given this, we can run LDA right out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "025OyjO8_ubT"
   },
   "outputs": [],
   "source": [
    "# As of July 2019, gensim calls a deprecated numpy function and gives lots of warning messages\n",
    "# Let's supress these.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# run LDA on our corpus, using out dictionary (k=6)\n",
    "lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=6)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1ZYzpEnVivz"
   },
   "source": [
    "- There is still quite a bit of noise in this list because the documents are full of very common words like \"write\", \"subject\", and \"from\".\n",
    "- One common approach is to remove the most (and possibly least) common words before running LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j33bfjEGZesn"
   },
   "outputs": [],
   "source": [
    "total_counts = np.sum(corpus_matrix, axis=0)\n",
    "sorted_words = sorted( zip( range(len(total_counts)) ,total_counts), \\\n",
    "                       key=lambda x:x[1], reverse=True )\n",
    "N = 100\n",
    "M = 50\n",
    "top_N_ids = [item[0] for item in sorted_words[:N]]\n",
    "appears_less_than_M_times = [item[0] for item in sorted_words if item[1] < M]\n",
    "vocab_dense = [id2word[idx] for idx in range(len(id2word))]\n",
    "\n",
    "print(\"Top words to remove:\", ' '.join([id2word[idx] for idx in top_N_ids]))\n",
    "\n",
    "remove_indexes = top_N_ids+appears_less_than_M_times\n",
    "corpus_matrix_filtered = np.delete(corpus_matrix,remove_indexes,1)\n",
    "\n",
    "for index in sorted(remove_indexes, reverse=True):\n",
    "    del vocab_dense[index]\n",
    "\n",
    "id2word_filtered = {}\n",
    "word2id_filtered = {}\n",
    "\n",
    "for i,word in enumerate(vocab_dense):\n",
    "    id2word_filtered[i] = word\n",
    "    word2id_filtered[word] = i\n",
    "    \n",
    "corpus_filtered = gensim.matutils.Dense2Corpus(corpus_matrix_filtered, documents_columns=False)\n",
    "\n",
    "print(\"Original matrix shape:\",corpus_matrix.shape)\n",
    "print(\"New matrix shape:\",corpus_matrix_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNLlfEeEifvb"
   },
   "source": [
    "- Now, run LDA again using this new matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zj6Ru2g8igC-"
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(corpus_filtered, id2word=id2word_filtered, num_topics=6)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qChk7x4jDbUA"
   },
   "source": [
    "- We can also use this model to get topic probabilities for unseen documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLEL1uyzDX-i"
   },
   "outputs": [],
   "source": [
    "unseen_doc = \"I went to the baseball game and say the player hit a homerun !\"\n",
    "unseen_doc_bow = [word2id_filtered.get(word.lower(),-1) for word in unseen_doc.split()]\n",
    "unseen_doc_vec = np.zeros(len(word2id_filtered))\n",
    "for word in unseen_doc_bow:\n",
    "    if word >= 0:\n",
    "        unseen_doc_vec[word] += 1\n",
    "unseen_doc_vec = unseen_doc_vec[np.newaxis]\n",
    "unseen_doc_corpus = gensim.matutils.Dense2Corpus(unseen_doc_vec, documents_columns=False)\n",
    "vector = lda[unseen_doc_corpus]  # get topic probability distribution for a document\n",
    "for item in vector:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbEN3OK9DRPh"
   },
   "source": [
    "- pyLDAvis is a nice tool for visualizing our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RnbNcMGDU-e"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# need to create a gensim dictionary object instead of our\n",
    "# lightweight dict object - this is what pyLDA expects as input\n",
    "dictionary = gensim.corpora.Dictionary()\n",
    "dictionary.token2id = word2id_filtered\n",
    "\n",
    "# visualize the LDA model\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus_filtered, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvbkZG1a7LOL"
   },
   "source": [
    "### Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La8mzCFvLaN8"
   },
   "source": [
    "- For a fast and easy content analysis, use one of the many available prebuilt dictionaries/lexicons.\n",
    "    - These map words or stems to semantic categories.\n",
    "- We have discussed several lexicons in the slides.\n",
    "- As an example, let's load the lexicon for measuring personal values in text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wFgdmY-7Q3-"
   },
   "outputs": [],
   "source": [
    "def load_lexicon(lexicon_file_path):\n",
    "    word2cat = collections.defaultdict(list)\n",
    "    with open(lexicon_file_path,'r') as lexicon_file:\n",
    "        for line in lexicon_file:\n",
    "            if line:\n",
    "                word, cat = line.strip().split(\", \")\n",
    "                word2cat[word].append(cat)\n",
    "    return word2cat\n",
    "            \n",
    "values_lexicon = load_lexicon(\"values_lexicon.txt\")\n",
    "print(\"Loaded lexicon with\",len(values_lexicon),\"entries.\")\n",
    "print(\"The categories for 'mother' are:\",values_lexicon['mother'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCHhIR-ILxjY"
   },
   "source": [
    "It's very easy to score a document for each category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXvCQuoWJmnY"
   },
   "outputs": [],
   "source": [
    "file_list = [\"christian_500.txt\", \"business_500.txt\", \"college_500.txt\"]\n",
    "\n",
    "for file_name in file_list:\n",
    "    category_counts = collections.defaultdict(int)\n",
    "    \n",
    "    # just look at the first 25K characters\n",
    "    # this way, we don't need to normalize based on the length of the document\n",
    "    # and we'll save some time since this is just for demonstration purposes\n",
    "    text = open(file_name).read().lower()[:25000]\n",
    "    for pattern, categories in values_lexicon.items():\n",
    "        count = re.findall(r'\\b' + pattern + r'\\b', text)\n",
    "        if count:\n",
    "            for category in categories:\n",
    "                category_counts[category] += len(count)\n",
    "    print(file_name,sorted(category_counts.items(),key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrfyoMOW8Wlc"
   },
   "source": [
    "### Putting it together: analyzing social media content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtePbGMpQOSo"
   },
   "source": [
    "- Let's try out some content analysis on the corpus that you created.\n",
    "    - Keep in mind any characteristics of the data that may pollute the content, such as \"RT\" at the beginning of a tweet, that you may want to filter out.\n",
    "- Here is some sample code to load a file from your computer. Go ahead and run it to upload your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCQpn17K8aQH"
   },
   "outputs": [],
   "source": [
    "# Load your social media data that you created in the previous section.\n",
    "uploaded = files.upload()\n",
    "print(\"Uploaded\",len(uploaded),\"files.\")\n",
    "for filename in uploaded:\n",
    "    print(\"File:\",filename)\n",
    "    # access file data here\n",
    "    with open(filename) as file_handle:\n",
    "        data = file_handle.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HgtKkZbQ0Ek"
   },
   "source": [
    "**Exercise 5:** Social Media Analysis\n",
    "\n",
    "- Run LDA on your own data\n",
    "    - Write some code to convert your data into a gensim corpus\n",
    "    - This should involve:\n",
    "        - parsing the input based on the format that you decided upon when you saved the file in the previous section.\n",
    "        - preprocessing the data (you can do this however you like, perhaps differently than we did before -- it's up to you), which might include:\n",
    "            - punctuation removal\n",
    "            - sentence and word tokenization\n",
    "            - cleaning the data (e.g., spelling correction, converting emoji, removing links)\n",
    "            - lemmatization or stemming (if you choose)\n",
    "        - generating a vocab and a count matrix\n",
    "        - creating the gensim corpus object\n",
    "- This one may take a bit more code than the previous exercises.\n",
    "    - It may be a good idea to split your code into separate functions.\n",
    "    - Feel free to search around for documentation or examples. You can use different functions or approaches than we used in the other sections if you find a methodology that you prefer.\n",
    "    - Of course, you can write and run all the code on your own machine if you don't want to do it all in colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46fQs2hQSQwT"
   },
   "outputs": [],
   "source": [
    "# load the data into a list of documents\n",
    "\n",
    "# preprocess each document\n",
    "\n",
    "# create a gensim corpus object and id2word dictionary\n",
    "\n",
    "corpus = None\n",
    "id2word = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pU_YaXh1L8Nz"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "# This is based on the format used to store the output in the \n",
    "# previous sample solution\n",
    "docs = open(\"mytweets.txt\").read().split(\"</TWEET>\\n<TWEET>\")\n",
    "docs[0] = docs[0].replace(\"<TWEET>\",\"\")\n",
    "docs[1] = docs[1].replace(\"</TWEET>\",\"\")\n",
    "\n",
    "# preprocess each document\n",
    "\n",
    "corpus_matrix, word2id = docs2matrix(docs)\n",
    "\n",
    "# create a gensim corpus object and id2word dictionary\n",
    "\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "# Dense2Corpus expects that each \n",
    "corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)\n",
    "print(\"Loaded\",len(corpus),\"documents into a Gensim corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TYRQonqMS-0h"
   },
   "source": [
    "- Now that you have the corpus object, let's fit and LDA model and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HvbTLhlTEI3"
   },
   "outputs": [],
   "source": [
    "# Change this, if you like\n",
    "k = 10\n",
    "\n",
    "lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=k)\n",
    "lda.print_topics()\n",
    "\n",
    "# need to create a gensim dictionary object instead of our\n",
    "# lightweight dict object - this is what pyLDA expects as input\n",
    "dictionary = gensim.corpora.Dictionary()\n",
    "dictionary.token2id = word2id\n",
    "\n",
    "# visualize the LDA model\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5lqJ1jURAGm"
   },
   "source": [
    "- If you still have time, try running the values lexicon on your data.\n",
    "    - Think about how you can use the corpus_matrix that you (may) have already created to easily get the counts of each category for each document.\n",
    "    - Which document has the highest score for each value category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5uVo6L2RJ7k"
   },
   "outputs": [],
   "source": [
    "# Bonus activity workspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7ZAPyi4yJrP"
   },
   "source": [
    "- [-> Next: Text Embeddings](https://colab.research.google.com/github/gordeli/textanalysis/blob/master/colab/05_Text_Embeddings_DS3Text.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "04_Content_Analysis_DS3Text.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
