{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/textanalysis/blob/master/03_Data_Collection_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Fundamentals of Text Analysis for User Generated Content @ EDHEC, 2021\n",
    "\n",
    "# Part 3: Data Collection\n",
    "\n",
    "[<- Previous: Noisy Text Processing](https://colab.research.google.com/github/gordeli/textanalysis/blob/master/colab/02_Text_Processsing_Basics_DS3Text.ipynb)\n",
    "\n",
    "[-> Next: Content Analysis](https://colab.research.google.com/github/gordeli/textanalysis/blob/master/colab/04_Text_Processsing_Basics_DS3Text.ipynb)\n",
    "\n",
    "Dates: February 8 - 15, 2021\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKVEnPi34qj4"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# For processing the incoming Twitter data\n",
    "import json\n",
    "# os-level utils\n",
    "import os\n",
    "# For downloading web data\n",
    "import requests\n",
    "# For compressing files\n",
    "import zipfile\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# beautiful soup for html parsing\n",
    "!pip install beautifulsoup4\n",
    "import bs4\n",
    "\n",
    "# tweepy for using the Twitter API\n",
    "!pip install tweepy\n",
    "import tweepy\n",
    "\n",
    "\n",
    "# allows downloading of files from colab to your computer\n",
    "from google.colab import files\n",
    "\n",
    "# get sample reddit data\n",
    "if not os.path.exists(\"reddit_2019_05_5K.json\"):\n",
    "    !wget https://raw.githubusercontent.com/gordeli/textanalysis/master/data/reddit_2019_05_5K.json\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyPPpRkI6i0n"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMYV2R0h6DTN"
   },
   "source": [
    "- Here we'll cover a few different sources of user-generated content and provide some examples of how to gather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZhKTVFO6q2b"
   },
   "source": [
    "### Web Scraping and HTML parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeBSAsuOHJ3Z"
   },
   "source": [
    "- Lots of text data is available directly from web pages.\n",
    "- Have a look at the following website: [Quotes to Scrape](http://quotes.toscrape.com/page/1/)\n",
    "- With the Beautiful Soup library, it's very easy to take some html and extract only the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TA4bHC-06uXb"
   },
   "outputs": [],
   "source": [
    "html_content = requests.get(\"http://quotes.toscrape.com/page/1/\").content\n",
    "soup = bs4.BeautifulSoup(html_content,\"html.parser\")\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXKZcbEIHjOW"
   },
   "source": [
    "- If you want to extract data in a more targeted way, you can navitage the [html document object model](https://www.w3schools.com/whatis/whatis_htmldom.asp) using [Beautiful Soup functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), but we won't dive deeply into this for now,\n",
    "- **Important: You should not use this kind of code to just go collect data from any website!**\n",
    "    - Web scaping tools should always check a site's [`robots.txt` file](https://www.robotstxt.org/robotstxt.html), which describes how crawlers, scrapers, indexers, etc., should use the site.\n",
    "        - For example, see [github's robots.txt](https://github.com/robots.txt)\n",
    "    - You should be able to find any site's robots.txt (if there is one) at http://\\<domain\\>/robots.txt for any web \\<domain\\>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be60X7Eu6zd2"
   },
   "source": [
    "### Reddit Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAWPIB1E_rU9"
   },
   "source": [
    "- Reddit is a great source of publicly available user-generated content.\n",
    "- We could scrape Reddit ourselves, but why do that if someone has already (generously) done the heavy lifting?\n",
    "- Reddit user Stuck_in_the_Matrix has compiled and compressed essentially all of Reddit for researchers to download.\n",
    "- [Original submissions corpus](https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/) (up to 2015) and [updates](https://files.pushshift.io/reddit/submissions/) (up to 5/2019 at the time of creating this notebook).\n",
    "    - For a smaller file to get started with, take a look at the [daily comments files](https://files.pushshift.io/reddit/comments/daily/).\n",
    "    - To explore more files available, see [this top-level directory](https://files.pushshift.io/reddit/).\n",
    "- Let's explore a small subset of the data from May 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3KftObP6_Rb"
   },
   "outputs": [],
   "source": [
    "# read the data that was downloaded during setup\n",
    "# this is the exact format as the full corpus, just truncated to the first 5000 lines\n",
    "sample_reddit_posts_raw = open(\"reddit_2019_05_5K.json\",'r').readlines()\n",
    "print(\"Loaded\",len(sample_reddit_posts_raw),\"reddit posts.\")\n",
    "reddit_json = [json.loads(post) for post in sample_reddit_posts_raw]\n",
    "print(json.dumps(reddit_json[50], sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXB5iK87hLtg"
   },
   "source": [
    "- Since the posts are in json format, we used the Python json library to process them.\n",
    "    - This library returns Python dict objects, so we can access them just like we would any other dictionary.\n",
    "- Let's view some of the text content from these posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTQbFr4zdsh_"
   },
   "outputs": [],
   "source": [
    "for post in reddit_json[:100]:\n",
    "    if post['selftext'].strip() and post['selftext'] not in [\"[removed]\",\"[deleted]\"]:\n",
    "        print(\"Subreddit:\",post['subreddit'],\"\\nTitle:\",post['title'],\"\\nContent:\", \\\n",
    "              post['selftext'],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-dIHIU2gs3K"
   },
   "source": [
    "- Note that we filtered out posts with no text content.\n",
    "    - Many posts have a non-null \"media\" field, which could contain images, links to youtube, videos, etc.\n",
    "        - These could be worth exploring more, using computer vision to process images/videos and NLP to process linked websites.\n",
    "- That covers the basics of getting Reddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxyARJY46vKO"
   },
   "source": [
    "### The Twitter API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkPEhJn_Kga0"
   },
   "source": [
    "- Twitter is also known for being an abundant source of publc text data (perhaps even more so than Reddit).\n",
    "- Twitter provides several types of API that can be used to collect anything from tweets to user descriptions to follower networks.\n",
    "    - You can [read all about it here](https://developer.twitter.com/).\n",
    "- For this tutorial, we'll look at using the [standard search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html), which allows us to retreive tweets that contain specific words, phrases, and hashtags.\n",
    "- In the slides, we talked about how to setup a Twitter App and get a API keys.\n",
    "    - You should add your own keys below and then run the code block to set your keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdINIx1PLunM"
   },
   "outputs": [],
   "source": [
    "twitter_API_key = \"\"\n",
    "twitter_API_secret_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8unaxcDL7gT"
   },
   "source": [
    "- Do not share your credentials with anyone!\n",
    "    - You shouldn't hardcode your API keys in code (like above) if you are going to save the file anywhere that is visible to others (like commiting the file to github).\n",
    "        - You can read more about securing your API keys [here](https://developer.twitter.com/en/docs/basics/authentication/guides/securing-keys-and-tokens).\n",
    "     - So, if you plan to save this file in any way, make sure to remove your API keys first.\n",
    "     - If you think your keys have been compromized, you can regenerate them.\n",
    "        - [Apps](https://developer.twitter.com/en/apps) -> Keys and Tokens -> Regenerate\n",
    "- Now, let's see how we can use the [tweepy](https://github.com/tweepy/tweepy) library to collect some tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHqPNZl-6yQU"
   },
   "outputs": [],
   "source": [
    "# create an auth handler object using the api tokens\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "\n",
    "# tweepy automatically takes care of potential rate limiting issues\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# let's look for some tweets\n",
    "query = \"#WomensWorldCup\"\n",
    "\n",
    "# count: 100 is the max allowed value for this parameter\n",
    "#     though we might get fewer than that\n",
    "# tweet_mode: Twitter changed the char limit from 140->280, but didn't want\n",
    "#     to break applications expecting 140, so we have to make sure to ask for this.\n",
    "tweets = API.search(q=query, count=100, tweet_mode=\"extended\")\n",
    "\n",
    "print(\"Collected\",len(tweets),\"tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuZytN78mSqB"
   },
   "source": [
    "- Great, hopefully you got some tweets! Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYONLu2mmR0L"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(tweets[0]._json, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EMpra6doeC6"
   },
   "source": [
    "- Here is the text portion of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FakwVnGUoR-9"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\".join([tweet.full_text for tweet in tweets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEWIjJ3iooyB"
   },
   "source": [
    "- Things are starting to look a bit more like our examples from the noisy text section.\n",
    "- To make it even easier to collect tweets from page to page, we can use the tweepy Cursor object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AT4f7mqTw-m_"
   },
   "outputs": [],
   "source": [
    "cursor = tweepy.Cursor(API.search, q=\"#Paris\", tweet_mode=\"extended\")\n",
    "\n",
    "# just get 5 tweets\n",
    "# if not given, will (in theory) retrieve as many matching tweets as possible\n",
    "# (standard search only allows search within previous ~1 week)\n",
    "for tweet in cursor.items(5):\n",
    "    print(tweet.full_text)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Lyt5mwD9AH7"
   },
   "source": [
    "### Putting it together: building your own corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruX8QSyVqr09"
   },
   "source": [
    "**Exercise 4:** Tweet collection\n",
    "\n",
    "- Let's write a function to collect a larger set of tweets related to a query\n",
    "    - If you want to collect data using multiple queries, you can just call this function multiple times, changing the query each time.\n",
    "    - Store the tweets in the file howerever you like\n",
    "        - You will need to write your own parser for this file later on in the tutorial.\n",
    "    - Store whatever information you like about each tweet, but collect the `full_text` at the very least.\n",
    "    - Make sure to check if `limit` is set, and if it is, only collect `limit` tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45JYj1e-9O9i"
   },
   "outputs": [],
   "source": [
    "def write_tweets_to_file(API, query, output_filename, limit=5):\n",
    "# ------------- Exercise 3 -------------- #\n",
    "    # gather tweets here, then write to output_filename\n",
    "# ---------------- End ------------------ #\n",
    "\n",
    "# quick test\n",
    "query = \"#twitter\"\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "output_filename = \"test.txt\"\n",
    "write_tweets_to_file(auth, query, output_filename, limit=3)\n",
    "print(\"Wrote this to the file:\",'\\n'+open(output_filename).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6wFhuwcrdz7"
   },
   "source": [
    "- Now, change the `query` string below to whatever you like, and run the code.\n",
    "    - *Make sure your code above is working before you run this! Otherwise, you may run quite a few queries and hit your rate limit, preventing you from testing your code again for ~15 minutes*\n",
    "    - See [this page](https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html) under \"standard search operators\" for details on what kinds of things you can place here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjmMnYD5o8_D"
   },
   "outputs": [],
   "source": [
    "query = \"#DataScience\"\n",
    "\n",
    "# call the tweet collection function\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "output_filename = \"mytweets.txt\"\n",
    "write_tweets_to_file(API, query, output_filename, 10000)\n",
    "\n",
    "# zip and download\n",
    "output_zip = output_filename + '.zip'\n",
    "with zipfile.ZipFile(output_zip, 'w') as myzip:\n",
    "    myzip.write(output_filename)\n",
    "files.download(output_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjgKtTebEM2w"
   },
   "source": [
    "- Note: with some web browsers, the `files.download()` command won't correctly open a dialog window to download the files.\n",
    "    - If this happens, check out the \"Files\" menu on the sidebar\n",
    "        - can be expanded on the left side of this notebook -- click the > button in the top left-corner to unhide the menu.\n",
    "    - You can download your file there (and also upload it when you need it in the next notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjmRRgzmyIXH"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "def write_tweets_to_file(api, query, output_filename, limit=10):\n",
    "    cursor = tweepy.Cursor(API.search, q=query, tweet_mode=\"extended\")\n",
    "    with open(output_filename,'w') as out:\n",
    "        for tweet in cursor.items(limit):\n",
    "            # using tags since tweets may have newlines in them\n",
    "            # you may also want to write other information to this file,\n",
    "            # or even the entire json object.\n",
    "            out.write('<TWEET>' + tweet.full_text + '</TWEET>\\n')\n",
    "            \n",
    "# quick test\n",
    "query = \"#twitter\"\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "output_filename = \"test2.txt\"\n",
    "write_tweets_to_file(auth, query, output_filename, limit=3)\n",
    "print(\"Wrote this to the file:\",'\\n'+open(output_filename).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FmOf-jZ1F-k"
   },
   "source": [
    "- You should now have your own file(s) containing Twitter data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etUZgi-twrES"
   },
   "source": [
    "- [-> Next: Content Analysis](https://colab.research.google.com/github/gordeli/textanalysis/blob/master/colab/04_Text_Processsing_Basics_DS3Text.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_Data_Collection_DS3Text.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
